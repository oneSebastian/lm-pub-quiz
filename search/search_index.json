{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started","text":"<p>This library implements a knoweledge probing approach which uses LM's inherent ability to estimate the log-likelihood of any given textual statement. For more information visit the LM Pub Quiz website.</p> <p>In this guide, we will first cover how to set up the packaged (as there are multiple options). For information on how to use the pacakge, see an example workflow using the API or the CLI-guide.</p>"},{"location":"#recommend-setup","title":"Recommend Setup","text":"<p>In this short guide, we explain how to setup your environment and run the first command.</p> <p>If you just want to use the CLI utilities, we recommend using <code>pipx</code>. If you want to make changes to the package (in order to contribute or customize the experiments), we recommend cloing it and use either an environment automatically managed by <code>hatch</code> or a manually managed environment.</p>"},{"location":"#install-in-a-manually-managed-environment","title":"Install in a manually managed environment","text":""},{"location":"#optional-set-up-your-desired-environment","title":"(Optional) Set up your desired environment","text":"<p>In this example we are using <code>conda</code> (and assume you want to install the CPU version of PyTorch; modify accordingly):</p> <pre><code># create an new environment\nconda create --name knowledge-probe\n\n# activate the new environment\nconda activate knowledge-probe\n\n# install pytorch in conda\nconda install pytorch cpuonly -c pytorch\n</code></pre>"},{"location":"#install-the-package","title":"Install the Package","text":"<p>You can install the package locally or directly from PyPI.</p>"},{"location":"#from-pypi","title":"From PyPI","text":"<pre><code>pip install lm-pub-quiz\n</code></pre>"},{"location":"#from-the-source-code","title":"From the Source Code","text":"<p>Alternatively, you can clone the repository, then (in your desired environment) run:</p> <pre><code>pip install -e lm-pub-quiz # local package (replace lm-pub-quiz with the path to the repository)\n</code></pre> <p>This allows you to make changes source code which will be reflected directly.</p>"},{"location":"#install-in-a-pipx-managed-environment","title":"Install in a pipx-managed environment","text":"<p>If you only want to use the command line interface, we recommend to install the package using <code>pipx</code>. With <code>pipx</code>, you can install the package directly using <code>pipx install lm-pub-quiz</code>. This will make the commands available on your system, but isolate all dependencies.</p>"},{"location":"#install-in-a-hatch-managed-environment","title":"Install in a hatch-managed environment","text":"<p>If you want to contribute to lm-pub-quiz, we recommend to use hatch. In this case you need to:</p> <ol> <li>Clone the repository,</li> <li>in the directoy run your respective commands in a hatch shell (either <code>hatch run &lt;command&gt;</code> or run <code>hatch shell</code> and continue your work there).</li> </ol> <p>This allows you to run the test cases by executing  To run the test cases, run <code>hatch run test</code> or <code>hatch run all:test</code> (to test on multiple python versions) and to check the formatting and correct typing using <code>hatch run lint:all</code>.</p>"},{"location":"#verify-the-installation","title":"Verify the Installation","text":"<p>To verify the package has been installed correctly, execute:</p> <pre><code>evaluate_model --help\n</code></pre> <p>You should than see an output similar to this:</p> <pre><code>$ evaluate_model --help\n\nusage: evaluate_model [-h] [config_file] &lt;configuration options to overwrite&gt;\n\nEvaluate a given model on a dataset.\n\npositional arguments:\n  config_file           Top-level config file to load (optional).\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --series-skip N       Skip first N trials in the execution of a series.\n\nconfiguration:\n  --model PATH\n  --model.name_or_path MODEL.NAME_OR_PATH\n  --model.tokenizer MODEL.TOKENIZER\n  --model.reduction MODEL.REDUCTION\n  --model.pll_metric MODEL.PLL_METRIC\n  --dataset PATH\n  --dataset.path DATASET.PATH\n  --output_base_path OUTPUT_BASE_PATH\n</code></pre>"},{"location":"api/","title":"API Reference","text":"<p>You can use the API to call the evaluation from a python script. For this, you need to load a dataset (see Data Files for how these should be structured) and then execute the evaluation function using your desired configuration.</p> <p>Example (compare with <code>src/lm_pub_quiz/cli/evaluate_model.py</code>):</p> <pre><code>from lm_pub_quiz import Dataset, Evaluator\n\n# Load dataset\ndataset = Dataset.from_path(\"data/BEAR\")\n\n# Create Evaluator (and load model)\nevaluator = Evaluator.from_model(\"distilbert-base-cased\")\n\n# Run evaluation\nresult = evaluator.evaluate_dataset(dataset)\n\n# Save result object\nresult.save(\"outputs/my_results\")\n</code></pre>"},{"location":"api/#evaluator","title":"Evaluator","text":""},{"location":"api/#lm_pub_quiz.Evaluator","title":"<code>lm_pub_quiz.Evaluator</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>Perplexity-based evaluator base class.</p>"},{"location":"api/#lm_pub_quiz.Evaluator.score_answers","title":"<code>score_answers(*, template, answers, reduction, subject=None)</code>  <code>abstractmethod</code>","text":"<p>Score an answer given a template.</p> <p>This function must be implemented by child-classes for each model-type.</p>"},{"location":"api/#lm_pub_quiz.MaskedLMEvaluator","title":"<code>lm_pub_quiz.MaskedLMEvaluator</code>","text":"<p>               Bases: <code>Evaluator</code></p>"},{"location":"api/#lm_pub_quiz.MaskedLMEvaluator.score_answers","title":"<code>score_answers(*, template, answers, reduction, subject=None)</code>","text":"<p>Calculates sequence scores using the Masked Language Model.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>The template to use (should contain a <code>[Y]</code> marker).</p> required <code>answers</code> <code>List[str]</code> <p>List of answers to calculate score for.</p> required <p>Returns:</p> Type Description <code>Union[ReducedReturnFormat, EachTokenReturnFormat]</code> <p>List[float]: List of suprisals scores per sequence</p>"},{"location":"api/#lm_pub_quiz.CausalLMEvaluator","title":"<code>lm_pub_quiz.CausalLMEvaluator</code>","text":"<p>               Bases: <code>Evaluator</code></p>"},{"location":"api/#lm_pub_quiz.CausalLMEvaluator.score_answers","title":"<code>score_answers(*, template, answers, reduction, subject=None)</code>","text":"<p>Calculates sequence scores using the Casual Language Model.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>The template to use (should contain a <code>[Y]</code> marker).</p> required <code>answers</code> <code>List[str]</code> <p>List of answers to calculate score for.</p> required <p>Returns:</p> Type Description <code>Union[EachTokenReturnFormat, ReducedReturnFormat]</code> <p>List[float]: List of suprisals scores per sequence</p>"},{"location":"api/#dataset-representation","title":"Dataset Representation","text":"<p>There are two classes which are used to represent a dataset: <code>Relation</code> and <code>Dataset</code> (which is essentially a container for a number of relations).</p>"},{"location":"api/#lm_pub_quiz.Relation","title":"<code>lm_pub_quiz.Relation</code>","text":"<p>               Bases: <code>RelationBase</code></p> <p>Represents a relation within a dataset, including its code, answer space, templates, and an instance table.</p> <p>Attributes:</p> Name Type Description <code>relation_code</code> <code>str</code> <p>A unique code identifying the relation.</p> <code>answer_space</code> <code>List[str]</code> <p>A list of possible answers for this relation.</p> <code>templates</code> <code>List[str]</code> <p>Templates for generating instances of this relation.</p> <code>instance_table</code> <code>DataFrame</code> <p>A pandas DataFrame containing instances of the relation.</p> <p>Methods:</p> Name Description <code>__str__</code> <p>Returns a string representation showing the first five instances in the relation.</p> <code>__repr__</code> <p>Returns a string representation of the relation code.</p> <code>__len__</code> <p>Returns the number of instances in the relation.</p> <code>subsample</code> <p>Randomly samples a subset of instances from the relation.</p> <code>load_from_file</code> <p>Class method to create a Relation instance from a JSONL file.</p>"},{"location":"api/#lm_pub_quiz.Relation.from_path","title":"<code>from_path(path, *, relation_code=None, lazy=True, fmt=None)</code>  <code>classmethod</code>","text":"<p>Loads a relation from a JSONL file and associated metadata.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>The path to the dataset directory.</p> required <code>relation_code</code> <code>str</code> <p>The specific code of the relation to load.</p> <code>None</code> <code>lazy</code> <code>bool</code> <p>If False, the instance table is loaded directly into memory.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Relation</code> <code>Relation</code> <p>An instance of the Relation class populated with data from the file.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error in loading the file or processing the data.</p>"},{"location":"api/#lm_pub_quiz.Relation.subsample","title":"<code>subsample(n=10)</code>","text":"<p>Returns only a subsampled version of the dataset of the size n.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Size of the subsampled dataset</p> <code>10</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Subsampled version of the dataset.</p>"},{"location":"api/#lm_pub_quiz.Dataset","title":"<code>lm_pub_quiz.Dataset</code>","text":"<p>               Bases: <code>DatasetBase[Relation]</code></p> <p>A collection of relations forming a multiple choice dataset.</p> <p>Attributes:</p> Name Type Description <code>relations</code> <code>List[Relation]</code> <p>A list of Relation instances in the dataset.</p> <code>dataset_name</code> <code>str</code> <p>The name of the dataset.</p> <p>Methods:</p> Name Description <code>load_from_path</code> <p>Class method to load a dataset from a specified path.</p>"},{"location":"api/#lm_pub_quiz.Dataset.from_name","title":"<code>from_name(name, *, lazy=True, base_path=None, chunk_size=10 * 1024, relation_info=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Loads a dataset from the cache (if available) or the url which is specified in the internal dataset table.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the dataset.</p> required <code>lazy</code> <code>bool</code> <p>If False, the instance tables of all relations are directly loaded into memory.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>An instance if Dataset loaded with the relations from the directory.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error in loading the dataset.</p> Usage <p>Loading the BEAR-dataset. <pre><code>&gt;&gt;&gt; from lm_pub_quiz import Dataset\n&gt;&gt;&gt; dataset = Dataset.from_name(\"BEAR\")\n</code></pre></p>"},{"location":"api/#lm_pub_quiz.Dataset.from_path","title":"<code>from_path(path, *, lazy=True, fmt=None, relation_info=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Loads a multiple choice dataset from a specified directory path.</p> <p>This method scans the directory for relation files and assembles them into a MultipleChoiceDataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The directory path where the dataset is stored.</p> required <code>lazy</code> <code>bool</code> <p>If False, the instance tables of all relations are directly loaded into memory.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>An instance if Dataset loaded with the relations from the directory.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error in loading the dataset.</p> Usage <p>Loading the BEAR-dataset. <pre><code>&gt;&gt;&gt; from lm_pub_quiz import Dataset\n&gt;&gt;&gt; dataset = Dataset.from_path(\"/path/to/dataset/BEAR\")\n</code></pre></p>"},{"location":"api/#evaluation-result","title":"Evaluation Result","text":"<p>Similar to the dataset representation, the results are also represented in two classes <code>RelationResult</code> and the container <code>DatasetResults</code>.</p>"},{"location":"api/#lm_pub_quiz.RelationResult","title":"<code>lm_pub_quiz.RelationResult</code>","text":"<p>               Bases: <code>RelationBase</code></p>"},{"location":"api/#lm_pub_quiz.RelationResult.from_path","title":"<code>from_path(path, *, relation_code=None, metadata=None, lazy=True, fmt=None)</code>  <code>classmethod</code>","text":"<p>Loads the evaluated relation from a JSONL file and associated metadata.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>The path to the relations instance table.</p> required <p>Returns:</p> Name Type Description <code>RelationResult</code> <code>RelationResult</code> <p>An instance of the RelationResult class populated with data from the file.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error in loading the file or processing the data.</p>"},{"location":"api/#lm_pub_quiz.DatasetResults","title":"<code>lm_pub_quiz.DatasetResults</code>","text":"<p>               Bases: <code>DatasetBase[RelationResult]</code></p> <p>Container for relation results.</p>"},{"location":"api/#lm_pub_quiz.DatasetResults.from_path","title":"<code>from_path(path, *, lazy=True, fmt=None, relation_info=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Loads a results from a specified directory path.</p> <p>This method scans the directory for relation files and assembles them into a DatasetResults.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The directory path where the dataset is stored.</p> required <p>Returns:</p> Name Type Description <code>DatasetResults</code> <code>DatasetResults</code> <p>An instance of DatasetResults loaded with the results from the directory.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error in loading the dataset.</p> Usage <p>Loading all relation results for a dataset. <pre><code>from results import DatasetResults\nresults = DatasetResults.load_from_path('/path/to/results/', dataset_name='BEAR')\n</code></pre></p>"},{"location":"api/#lm_pub_quiz.DatasetResults.get_metadata","title":"<code>get_metadata(key=None)</code>","text":"<p>Return metadata from the relations. If no keys are passed, all consistent values are returned.</p>"},{"location":"api/#lm_pub_quiz.DatasetResults.get_metrics","title":"<code>get_metrics(metrics, *, accumulate=False, divide_support=True)</code>","text":"<p>Return the metrics for the relations in this dataset.</p> <p>Parameters:</p> Name Type Description Default <code>accumulate</code> <code>bool | str | None</code> <p>Compute the metrics for groups of relations (e.g. over the domains) or compute the overall scores for the complete dataset by setting <code>accumulate=True</code>.</p> <code>False</code> <code>divide_support</code> <code>bool</code> <p>Set to true to divide the support (added by a relation to a group) by the number of groups it adds to (only relevant if there are multiple groups per relation i.e. when <code>explode</code> is set). This leads to a dataframe where the weightted mean is equal to the overall score.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[DataFrame, Series]</code> <p>pandas.DataFrame | pandas.Series: A Series or DataFrame with the selected metrics depending on whether all relations where accumulated.</p>"},{"location":"api/#data-base-clasess","title":"Data Base Clasess","text":"<p>The dataset representations as well as the evaluation results are based on common base classes.</p>"},{"location":"api/#lm_pub_quiz.data.base.RelationBase","title":"<code>lm_pub_quiz.data.base.RelationBase</code>","text":"<p>               Bases: <code>DataBase</code></p> <p>Base class for the representation of relations and relations results.</p> Source code in <code>src/lm_pub_quiz/data/base.py</code> <pre><code>class RelationBase(DataBase):\n    \"\"\"Base class for the representation of relations and relations results.\"\"\"\n\n    _instance_table_file_name_suffix: str = \"\"\n    _instance_table_default_format: str = \"jsonl\"\n    _metadata_file_name: str = \"metadata_relations.json\"\n\n    _len: Optional[int] = None\n\n    def __init__(\n        self,\n        relation_code: str,\n        *,\n        lazy_options: Optional[Dict[str, Any]] = None,\n        instance_table: Optional[pd.DataFrame] = None,\n        answer_space: Optional[pd.Series] = None,\n        relation_info: Optional[Dict[str, Any]] = None,\n    ):\n\n        self._relation_code = relation_code\n        self._lazy_options = lazy_options\n        self._instance_table = instance_table\n        self._answer_space = answer_space\n        self._relation_info = relation_info or {}\n\n    @property\n    def relation_code(self) -&gt; str:\n        return self._relation_code\n\n    def copy(self, **kw):\n        \"\"\"Create a copy of the isntance with specified fields replaced by new values.\"\"\"\n\n        kw = {\n            \"relation_code\": self.relation_code,\n            \"lazy_options\": self._lazy_options.copy() if self._lazy_options is not None else None,\n            \"instance_table\": self._instance_table.copy() if self._instance_table is not None else None,\n            \"answer_space\": self._answer_space.copy() if self._answer_space is not None else None,\n            \"relation_info\": self._relation_info.copy(),\n            **kw,\n        }\n        return self.__class__(kw.pop(\"relation_code\"), **kw)\n\n    def saved(self, path: PathLike, *, fmt: InstanceTableFileFormat = None) -&gt; Self:\n        # Save relation and return the lazy-loading relation\n        saved_path = self.save(path, fmt=fmt)\n\n        if path is not None:\n            lazy_options = {\n                \"path\": saved_path,\n                \"fmt\": fmt,\n            }\n        else:\n            lazy_options = None\n\n        return self.copy(instance_table=None, lazy_options=lazy_options)\n\n    def activated(self) -&gt; Self:\n        \"\"\"Return self or a copy of self with the instance_table loaded (lazy loading disabled).\"\"\"\n\n        if not self.is_lazy:\n            return self\n\n        return self.copy(instance_table=self.instance_table)\n\n    def __repr__(self) -&gt; str:\n        return str(self)\n\n    def __str__(self) -&gt; str:\n        return f\"{self.__class__.__name__} `{self.relation_code}`\"\n\n    def get_metadata(self) -&gt; Dict[str, Any]:\n        if self._answer_space is not None:\n            return {\n                \"answer_space_labels\": self.answer_space.tolist(),\n                \"answer_space_ids\": self.answer_space.index.tolist(),\n                \"relation_info\": self._relation_info.copy(),\n            }\n        else:\n            return {}\n\n    @property\n    def _derived_cardinality(self) -&gt; str:\n        if self.instance_table.duplicated(\"obj_id\").any():\n            return \"multiple instances per answer\"\n        else:\n            return \"single instance per answer\"\n\n    @overload\n    def relation_info(self, /, **kw) -&gt; Dict[str, Any]: ...\n\n    @overload\n    def relation_info(self, key: str, /) -&gt; Any: ...\n\n    def relation_info(self, key: Optional[str] = None, /, **kw) -&gt; Union[None, Any, Dict[str, Any]]:\n        \"\"\"Get or set additional relation information.\"\"\"\n        if key is not None:\n            if key == \"cardinality\" and \"cardinality\" not in self._relation_info:\n                return self._derived_cardinality\n            else:\n                return self._relation_info[key]\n        elif len(kw) &gt; 0:\n            self._relation_info.update(kw)\n\n        info = self._relation_info.copy()\n        if \"cardinality\" not in info:\n            info[\"cardinality\"] = self._derived_cardinality\n        return info\n\n    @staticmethod\n    def _generate_obj_ids(n: int, *, id_prefix: str = \"\"):\n        return id_prefix + pd.RangeIndex(n, name=\"obj_id\").astype(str)\n\n    @classmethod\n    def answer_space_from_instance_table(cls, instance_table: pd.DataFrame, **kw) -&gt; pd.Series:\n        if \"obj_label\" not in instance_table:\n            msg = \"Cannot generate answer space: No object information in instance table.\"\n            raise ValueError(msg)\n\n        if \"obj_id\" in instance_table:\n            answer_groups = instance_table.groupby(\"obj_id\", sort=False).obj_label\n            unique_ids = answer_groups.nunique().eq(1)\n\n            if not unique_ids.all():\n                ids = \", \".join(f\"'{v}'\" for v in unique_ids[~unique_ids].index)\n                log.warning(\"Some object IDs contain multiple labels: %s\", ids)\n\n            return answer_groups.first()\n\n        else:\n            answer_labels = instance_table[\"obj_label\"].unique()\n            return pd.Series(answer_labels, index=cls._generate_obj_ids(len(answer_labels), **kw), name=\"obj_label\")\n\n    @classmethod\n    def answer_space_from_metadata(cls, metadata, **kw) -&gt; Optional[pd.Series]:\n        if \"answer_space_labels\" in metadata and \"answer_space_ids\" in metadata:\n            if \"answer_space_labels\" in metadata:\n                answer_space_labels = metadata.pop(\"answer_space_labels\")\n            else:\n                answer_space_labels = metadata.pop(\"answer_space\")\n\n            answer_space_ids = metadata.pop(\"answer_space_ids\", None)\n\n            if answer_space_ids is None:\n                answer_space_ids = cls._generate_obj_ids(len(answer_space_labels), **kw)\n\n            index = pd.Index(answer_space_ids, name=\"obj_id\")\n\n            answer_space = pd.Series(answer_space_labels, index=index, name=\"obj_label\")\n\n            return answer_space\n        elif (\n            \"answer_space_labels\" not in metadata\n            and \"answer_space_ids\" not in metadata\n            and \"answer_space\" not in metadata\n        ):\n            return None\n        else:\n            warnings.warn(\n                \"To define an answer space in the medata data, specify `answer_space_ids` and \"\n                \"`answer_space_labels` (using answer space base on the instance table).\",\n                stacklevel=1,\n            )\n            return None\n\n    @property\n    def answer_space(self) -&gt; pd.Series:\n        if self._answer_space is None:\n            # invoke file loading to get answer space\n            _ = self.instance_table\n\n        return cast(pd.Series, self._answer_space)\n\n    @property\n    def instance_table(self) -&gt; pd.DataFrame:\n        if self._instance_table is None:\n            if self._lazy_options is None:\n                msg = (\n                    f\"Could not load instance table for {self.__class__.__name__} \"\n                    f\"({self.relation_code}): No path given.\"\n                )\n                raise NoInstanceTableError(msg)\n\n            instance_table = self.load_instance_table(answer_space=self._answer_space, **self._lazy_options)\n\n            if self._answer_space is None:\n                # store answer_space\n                self._answer_space = self.answer_space_from_instance_table(\n                    instance_table, id_prefix=f\"{self.relation_code}-\"\n                )\n\n            # store number of instances\n            self._len = len(instance_table)\n\n            return instance_table\n\n        return self._instance_table\n\n    def __len__(self) -&gt; int:\n        if self._instance_table is None:\n            if self._len is None:\n                # invoke file loading to get answer space\n                _ = self.instance_table\n            return cast(int, self._len)\n        else:\n            return len(self.instance_table)\n\n    @abstractmethod\n    def filter_subset(self, indices: Sequence[int], *, keep_answer_space: bool = False) -&gt; Self:\n        pass\n\n    @classmethod\n    def load_instance_table(\n        cls,\n        path: Path,\n        *,\n        answer_space: Optional[pd.Series] = None,  # noqa: ARG003\n        fmt: InstanceTableFileFormat = None,\n    ) -&gt; pd.DataFrame:\n        if not path.exists():\n            msg = f\"Could not load instance table for {cls.__name__}: Path `{path}` could not be found.\"\n            raise FileNotFoundError(msg)\n        elif not path.is_file():\n            msg = f\"Could not load instance table for {cls.__name__}: `{path}` is not a file.\"\n            raise RuntimeError(msg)\n\n        if fmt is None:\n            fmt = tuple(s[1:] for s in path.suffixes)\n        elif isinstance(fmt, str):\n            fmt = tuple(fmt.split(\".\"))\n\n        log.debug(\"Loading instance table (format=.%s) from: %s\", \".\".join(fmt), path)\n\n        if fmt == (\"jsonl\",):\n            instance_table = pd.read_json(path, lines=True)\n\n        elif fmt[0] == \"parquet\" and len(fmt) &lt;= 2:  # noqa: PLR2004\n            instance_table = pd.read_parquet(path)\n\n        else:\n            msg = f\"Format .{'.'.join(fmt)} not recognized: Could not load instances at {path}.\"\n            raise ValueError(msg)\n\n        if instance_table.index.name is None:\n            instance_table.index.name = \"instance\"\n\n        return instance_table\n\n    @classmethod\n    def save_instance_table(cls, instance_table: pd.DataFrame, path: Path, fmt: InstanceTableFileFormat = None):\n        \"\"\"Save instance table with the format determined by the path suffix.\n\n        Parameters:\n           instance_table (pd.DataFrame): The instances to save.\n           path (Path): Where to save the instance table. If format is not specified, the suffix is used to determined\n                        the format.\n           fmt (str): Which to save the instances in.\n        \"\"\"\n        if fmt is None:\n            fmt = tuple(s[1:] for s in path.suffixes)\n        elif isinstance(fmt, str):\n            fmt = tuple(fmt.split(\".\"))\n\n        if fmt == (\"jsonl\",):\n            instance_table.to_json(path, orient=\"records\", lines=True)\n\n        elif fmt[0] == \"parquet\" and len(fmt) &lt;= 2:  # noqa: PLR2004\n            compression: Optional[str]\n\n            if len(fmt) == 1:\n                compression = None\n            else:\n                compression = fmt[1]\n\n            instance_table.to_parquet(path, compression=compression)\n        else:\n            msg = f\"Format .{'.'.join(fmt)} not recognized: Could not save instances at {path}.\"\n            raise ValueError(msg)\n\n    @property\n    def is_lazy(self) -&gt; bool:\n        return self._instance_table is None and self._lazy_options is not None\n\n    @property\n    @abstractmethod\n    def has_instance_table(self) -&gt; bool:\n        pass\n\n    def save(self, save_path: PathLike, fmt: InstanceTableFileFormat = None) -&gt; Optional[Path]:\n        \"\"\"Save results to a file and export meta_data\"\"\"\n        save_path = Path(save_path)\n        save_path.mkdir(parents=True, exist_ok=True)\n\n        log.debug(\"Saving %s result to: %s\", self, save_path)\n\n        ### Metadata file -&gt; .json ###\n        if save_path.is_dir():\n            metadata_path = save_path / self._metadata_file_name\n        else:\n            metadata_path = save_path\n            save_path = save_path.parent\n\n        if metadata_path.exists():\n            with open(metadata_path) as file:\n                all_metadata = json.load(file)\n\n                if self.relation_code in all_metadata:\n                    log.warning(\"Overwriting metadata info for relation %s (%s)\", self.relation_code, save_path)\n        else:\n            all_metadata = {}\n\n        ### Store instance table to .jsonl file ###\n        if self.has_instance_table:\n            instances_path = self.path_for_code(save_path, self.relation_code, fmt=fmt)\n            self.save_instance_table(self.instance_table, instances_path, fmt=fmt)\n            log.debug(\"Instance table was saved to: %s\", instances_path)\n\n        else:\n            instances_path = None\n\n        all_metadata[self.relation_code] = self.get_metadata()\n\n        with open(metadata_path, \"w\") as file:\n            json.dump(all_metadata, file, indent=4, default=str)\n            log.debug(\"Metadata file was saved to: %s\", metadata_path)\n\n        return instances_path\n\n    @staticmethod\n    def true_stem(path: Path) -&gt; str:\n        return path.name.partition(\".\")[0]\n\n    @classmethod\n    def code_from_path(cls, path: Path) -&gt; str:\n\n        if not path.name.endswith(cls._instance_table_file_name_suffix):\n            msg = (\n                f\"Incorrect path for {cls.__name__} instance table \"\n                f\"(expected suffix {cls._instance_table_file_name_suffix}): {path}\"\n            )\n            raise ValueError(msg)\n        code = cls.true_stem(path)\n        if len(cls._instance_table_file_name_suffix) &gt; 0:\n            code = code[: -len(cls._instance_table_file_name_suffix)]\n        return code\n\n    @classmethod\n    def suffix_from_instance_format(cls, fmt: InstanceTableFileFormat = None) -&gt; str:\n        if fmt is None:\n            return cls._instance_table_default_format\n        elif isinstance(fmt, str):\n            return fmt\n        else:\n            return \".\".join(fmt)\n\n    @classmethod\n    def path_for_code(cls, path: Path, relation_code: str, *, fmt: InstanceTableFileFormat = None) -&gt; Path:\n        return path / f\"{relation_code}{cls._instance_table_file_name_suffix}.{cls.suffix_from_instance_format(fmt)}\"\n\n    @overload\n    @classmethod\n    def search_path(cls, path: Path, relation_code: None = None, fmt: InstanceTableFileFormat = None) -&gt; List[Path]: ...\n\n    @overload\n    @classmethod\n    def search_path(cls, path: Path, relation_code: str, fmt: InstanceTableFileFormat = None) -&gt; Path: ...\n\n    @classmethod\n    def search_path(\n        cls, path: Path, relation_code: Optional[str] = None, fmt: InstanceTableFileFormat = None\n    ) -&gt; Union[List[Path], Path, None]:\n        \"\"\"Search path for instance files.\"\"\"\n\n        if relation_code is not None and fmt is not None:\n            # Just look for the file\n            p = cls.path_for_code(path, relation_code, fmt=fmt)\n            if p.exists():\n                return p\n            else:\n                return None\n\n        if relation_code is None:\n            code = \".*\"\n        else:\n            code = re.escape(relation_code)\n\n        if fmt is None:\n            suffix = \".*\"\n        else:\n            suffix = cls.suffix_from_instance_format(fmt)\n\n        pattern = re.compile(f\"(?P&lt;relation_code&gt;{code}){cls._instance_table_file_name_suffix}.(?P&lt;suffix&gt;{suffix})\")\n\n        matches: Dict[str, List[Path]] = defaultdict(list)\n        for p in map(Path, os.scandir(path)):\n            if p.name == cls._metadata_file_name:\n                continue\n\n            match = re.fullmatch(pattern, p.name)\n\n            if match is not None:\n                matches[match.group(\"relation_code\")].append(p)\n\n        selected_paths = []\n        for code, matching_paths in matches.items():\n            if len(matching_paths) &gt; 1:\n                log.warning(\"Found multiple files for relation %: %s\", code, \", \".join(p.name for p in matching_paths))\n            selected_paths.append(matching_paths[0])\n\n        if relation_code is None:\n            return selected_paths\n        elif len(selected_paths) == 0:\n            return None\n        else:\n            return selected_paths[0]\n</code></pre>"},{"location":"api/#lm_pub_quiz.data.base.RelationBase.activated","title":"<code>activated()</code>","text":"<p>Return self or a copy of self with the instance_table loaded (lazy loading disabled).</p> Source code in <code>src/lm_pub_quiz/data/base.py</code> <pre><code>def activated(self) -&gt; Self:\n    \"\"\"Return self or a copy of self with the instance_table loaded (lazy loading disabled).\"\"\"\n\n    if not self.is_lazy:\n        return self\n\n    return self.copy(instance_table=self.instance_table)\n</code></pre>"},{"location":"api/#lm_pub_quiz.data.base.RelationBase.copy","title":"<code>copy(**kw)</code>","text":"<p>Create a copy of the isntance with specified fields replaced by new values.</p> Source code in <code>src/lm_pub_quiz/data/base.py</code> <pre><code>def copy(self, **kw):\n    \"\"\"Create a copy of the isntance with specified fields replaced by new values.\"\"\"\n\n    kw = {\n        \"relation_code\": self.relation_code,\n        \"lazy_options\": self._lazy_options.copy() if self._lazy_options is not None else None,\n        \"instance_table\": self._instance_table.copy() if self._instance_table is not None else None,\n        \"answer_space\": self._answer_space.copy() if self._answer_space is not None else None,\n        \"relation_info\": self._relation_info.copy(),\n        **kw,\n    }\n    return self.__class__(kw.pop(\"relation_code\"), **kw)\n</code></pre>"},{"location":"api/#lm_pub_quiz.data.base.RelationBase.relation_info","title":"<code>relation_info(key=None, /, **kw)</code>","text":"<p>Get or set additional relation information.</p> Source code in <code>src/lm_pub_quiz/data/base.py</code> <pre><code>def relation_info(self, key: Optional[str] = None, /, **kw) -&gt; Union[None, Any, Dict[str, Any]]:\n    \"\"\"Get or set additional relation information.\"\"\"\n    if key is not None:\n        if key == \"cardinality\" and \"cardinality\" not in self._relation_info:\n            return self._derived_cardinality\n        else:\n            return self._relation_info[key]\n    elif len(kw) &gt; 0:\n        self._relation_info.update(kw)\n\n    info = self._relation_info.copy()\n    if \"cardinality\" not in info:\n        info[\"cardinality\"] = self._derived_cardinality\n    return info\n</code></pre>"},{"location":"api/#lm_pub_quiz.data.base.RelationBase.save","title":"<code>save(save_path, fmt=None)</code>","text":"<p>Save results to a file and export meta_data</p> Source code in <code>src/lm_pub_quiz/data/base.py</code> <pre><code>def save(self, save_path: PathLike, fmt: InstanceTableFileFormat = None) -&gt; Optional[Path]:\n    \"\"\"Save results to a file and export meta_data\"\"\"\n    save_path = Path(save_path)\n    save_path.mkdir(parents=True, exist_ok=True)\n\n    log.debug(\"Saving %s result to: %s\", self, save_path)\n\n    ### Metadata file -&gt; .json ###\n    if save_path.is_dir():\n        metadata_path = save_path / self._metadata_file_name\n    else:\n        metadata_path = save_path\n        save_path = save_path.parent\n\n    if metadata_path.exists():\n        with open(metadata_path) as file:\n            all_metadata = json.load(file)\n\n            if self.relation_code in all_metadata:\n                log.warning(\"Overwriting metadata info for relation %s (%s)\", self.relation_code, save_path)\n    else:\n        all_metadata = {}\n\n    ### Store instance table to .jsonl file ###\n    if self.has_instance_table:\n        instances_path = self.path_for_code(save_path, self.relation_code, fmt=fmt)\n        self.save_instance_table(self.instance_table, instances_path, fmt=fmt)\n        log.debug(\"Instance table was saved to: %s\", instances_path)\n\n    else:\n        instances_path = None\n\n    all_metadata[self.relation_code] = self.get_metadata()\n\n    with open(metadata_path, \"w\") as file:\n        json.dump(all_metadata, file, indent=4, default=str)\n        log.debug(\"Metadata file was saved to: %s\", metadata_path)\n\n    return instances_path\n</code></pre>"},{"location":"api/#lm_pub_quiz.data.base.RelationBase.save_instance_table","title":"<code>save_instance_table(instance_table, path, fmt=None)</code>  <code>classmethod</code>","text":"<p>Save instance table with the format determined by the path suffix.</p> <p>Parameters:</p> Name Type Description Default <code>instance_table</code> <code>DataFrame</code> <p>The instances to save.</p> required <code>path</code> <code>Path</code> <p>Where to save the instance table. If format is not specified, the suffix is used to determined           the format.</p> required <code>fmt</code> <code>str</code> <p>Which to save the instances in.</p> <code>None</code> Source code in <code>src/lm_pub_quiz/data/base.py</code> <pre><code>@classmethod\ndef save_instance_table(cls, instance_table: pd.DataFrame, path: Path, fmt: InstanceTableFileFormat = None):\n    \"\"\"Save instance table with the format determined by the path suffix.\n\n    Parameters:\n       instance_table (pd.DataFrame): The instances to save.\n       path (Path): Where to save the instance table. If format is not specified, the suffix is used to determined\n                    the format.\n       fmt (str): Which to save the instances in.\n    \"\"\"\n    if fmt is None:\n        fmt = tuple(s[1:] for s in path.suffixes)\n    elif isinstance(fmt, str):\n        fmt = tuple(fmt.split(\".\"))\n\n    if fmt == (\"jsonl\",):\n        instance_table.to_json(path, orient=\"records\", lines=True)\n\n    elif fmt[0] == \"parquet\" and len(fmt) &lt;= 2:  # noqa: PLR2004\n        compression: Optional[str]\n\n        if len(fmt) == 1:\n            compression = None\n        else:\n            compression = fmt[1]\n\n        instance_table.to_parquet(path, compression=compression)\n    else:\n        msg = f\"Format .{'.'.join(fmt)} not recognized: Could not save instances at {path}.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/#lm_pub_quiz.data.base.RelationBase.search_path","title":"<code>search_path(path, relation_code=None, fmt=None)</code>  <code>classmethod</code>","text":"<p>Search path for instance files.</p> Source code in <code>src/lm_pub_quiz/data/base.py</code> <pre><code>@classmethod\ndef search_path(\n    cls, path: Path, relation_code: Optional[str] = None, fmt: InstanceTableFileFormat = None\n) -&gt; Union[List[Path], Path, None]:\n    \"\"\"Search path for instance files.\"\"\"\n\n    if relation_code is not None and fmt is not None:\n        # Just look for the file\n        p = cls.path_for_code(path, relation_code, fmt=fmt)\n        if p.exists():\n            return p\n        else:\n            return None\n\n    if relation_code is None:\n        code = \".*\"\n    else:\n        code = re.escape(relation_code)\n\n    if fmt is None:\n        suffix = \".*\"\n    else:\n        suffix = cls.suffix_from_instance_format(fmt)\n\n    pattern = re.compile(f\"(?P&lt;relation_code&gt;{code}){cls._instance_table_file_name_suffix}.(?P&lt;suffix&gt;{suffix})\")\n\n    matches: Dict[str, List[Path]] = defaultdict(list)\n    for p in map(Path, os.scandir(path)):\n        if p.name == cls._metadata_file_name:\n            continue\n\n        match = re.fullmatch(pattern, p.name)\n\n        if match is not None:\n            matches[match.group(\"relation_code\")].append(p)\n\n    selected_paths = []\n    for code, matching_paths in matches.items():\n        if len(matching_paths) &gt; 1:\n            log.warning(\"Found multiple files for relation %: %s\", code, \", \".join(p.name for p in matching_paths))\n        selected_paths.append(matching_paths[0])\n\n    if relation_code is None:\n        return selected_paths\n    elif len(selected_paths) == 0:\n        return None\n    else:\n        return selected_paths[0]\n</code></pre>"},{"location":"api/#lm_pub_quiz.data.base.DatasetBase","title":"<code>lm_pub_quiz.data.base.DatasetBase</code>","text":"<p>               Bases: <code>DataBase</code>, <code>Generic[RT]</code></p> <p>Base class for a collection of relations or relations results.</p>"},{"location":"cli/","title":"Command Line Interface","text":"<p>There are three commands introduced by this package:</p> Command Description <code>score_sentence</code> A thin wrapper around <code>minicons</code> package -- scores a given sentence. <code>rank_answers</code> Rank answers for a given template. <code>evaluate_dataset</code> Run a complete evaluation on a given dataset. <p>Run a command with <code>--help</code> to get a description of the command and all its configuration options.</p>"},{"location":"cli/#configuration","title":"Configuration","text":"<p>It is possible to load a configuration file specifying the complete or parital configuration and overwrite configurations using the command line options (using two leading dashes, e.g. <code>--device DEVICE_TO_USE</code>).</p> <p>All commands share the same base arguments to specify the model and retrieval details:</p> Option Description <code>model.name_or_path</code> The model to use (can be a huggingface name of a local path; mandatory). <code>model.tokenizer</code> The name of the tokenizer to use (defaults to one matching the model). <code>model.reduction</code> Type of score reduction (can be <code>mean</code> or <code>sum</code>; defaults to the latter). <code>model.pll_metric</code> The type of PLL metric to use (only use when using MLM-type model; defaults to <code>within_word_l2r</code>). <code>model.lm_type</code> If the model type cannot be inferred, pass <code>MLM</code> or <code>CLM</code> depending on the type of your model. <code>model</code> Specify a path to load these (the <code>model.</code>) configurations from (the <code>model.</code> prefix must then be omitted). <code>device</code> Specify which device to use (default to using the CPU). <p>Each command has a set of additional options to specify what to score/evaluate/rank.</p>"},{"location":"cli/#configuration-files","title":"Configuration Files","text":"<p>Configuration files can be in either of the following formats (the extension must specify the format):</p> <ul> <li>JSON (<code>.jons</code>)</li> <li>YAML (<code>.yaml</code> or <code>.yml</code>)</li> <li>TOML (<code>.toml</code>)</li> </ul>"},{"location":"cli/#evaluate_dataset","title":"<code>evaluate_dataset</code>","text":"Option Description <code>dataset_path</code> Path to the dataset which is used for the evaluation. <code>batch_size</code> How many sentences score per batch. <code>output_base_path</code> Base path for storing the results. <code>debug</code> Set <code>debug</code> to true (or use <code>--debug</code>) to only evaluate two random instances per relation."},{"location":"cli/#rank_answers","title":"<code>rank_answers</code>","text":"Option Description <code>template</code> The template to use. Must be of the form \"Some sentence with a [Y] marker\". \"[Y]\" wil be replaced by each answer. <code>answers</code> A list of strings separated by ','."},{"location":"cli/#score_sentence","title":"<code>score_sentence</code>","text":"Option Description <code>sentence</code> The sentence to score."},{"location":"cli/#examples","title":"Examples","text":"<pre><code>$ score_sentence --model.name \"distilbert-base-cased\" --sentence \"The traveler lost the souvenir.\" --model.reduction \"none\"\nRank    Score   \n----------------\nThe         3.26\ntravel      8.34\n##er        3.81\nlost        8.16\nthe         2.18\nso          8.66\n##uve       3.54\n##nir      -0.00\n.           1.51\n\n$ score_sentence --model.name \"distilbert-base-cased\" --sentence \"The traveler lost the souvenir.\" --model.reduction \"none\" --model.pll_metric \"original\"\nRank    Score   \n----------------\nThe         3.26\ntravel      3.46\n##er        3.81\nlost        8.16\nthe         2.18\nso          0.03\n##uve       0.00\n##nir      -0.00\n.           1.51\n</code></pre>"},{"location":"data_files/","title":"Data Files","text":"<p>A project dataset comprises a series of <code>.jsonl</code> files and a single <code>.json</code> metadata file. These files collectively encapsulate relational knowledge relations. TREx dataset is included for demonstration purposes.</p>"},{"location":"data_files/#relation-data-jsonl-files","title":"Relation data: JSONL Files","text":"<p>Each <code>.jsonl</code> file represents relational data points (triple). The format for each line in these files consists of:</p> <ul> <li>predicate_id: A unique identifier (Wikidata) for the relation.</li> <li>sub_id: A unique identifier (Wikidata) for the subject entity.</li> <li>sub_label: A readable label or name for the subject entity.</li> <li>obj_id: A unique identifier (Wikidata) for the object entity.</li> <li>obj_label: A readable label or name for the object entity.</li> </ul>"},{"location":"data_files/#example-instance","title":"Example Instance:","text":"<pre><code>{\n  \"predicate_id\":\"P30\",\n  \"sub_id\":\"Q3108669\",\n  \"sub_label\":\"Lavoisier Island\",\n  \"obj_id\":\"Q51\",\n  \"obj_label\":\"Antarctica\"\n}\n</code></pre>"},{"location":"data_files/#metadata-json-file","title":"Metadata JSON File","text":"<p>The <code>metadata_relations.json</code> file provides contextual and auxiliary information about each relation (identified by the <code>predicate_id</code>) available in the <code>.jsonl</code> files. It specifies:</p> <ul> <li>templates: A list of sentence templates that can be used to form human-readable statements from triples in a given relation.</li> <li>answer_space: A list of possible answers that can be associated with the particular relation (set of all answers).</li> </ul>"},{"location":"data_files/#example-metadata","title":"Example Metadata:","text":"<pre><code>{\n  \"P30\": {\n    \"templates\": [\"[X] is located in [Y].\"],\n    \"answer_space\": [\"Asia\", \"Antarctica\", \"Africa\", \"Europe\", \"Americas\", \"Oceania\"]\n  }\n}\n</code></pre>"},{"location":"example/","title":"Example Workflow","text":"<p>In this short guide we will go through a brief workflow example, in which we run the BEAR probe on the gpt2 model and look at some results using the python api.</p>"},{"location":"example/#run-bear-probe-on-a-given-model","title":"Run BEAR probe on a given model","text":"<p>First we run the BEAR probe on the given model and save its results to our file system.</p> <pre><code>from lm_pub_quiz import Dataset, Evaluator\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load the BEAR dataset from its specific location\ndataset = Dataset.from_path(\"&lt;BEAR data path, e.g. ./transformer-knowledge-probe/data/BEAR&gt;\")\n\n# Load the gpt2 model\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(\"cuda\")\n\n\n# Run the BEAR evaluator and save the results\nresult_save_path = \"&lt;BEAR results save path&gt;\"  # doctest: +SKIP\n\n# In case the tokenizer cannot be loaded from the model directory, it may be loaded explicitly and passed to the Evaluator.from_model method via the 'tokenizer=' keyword\nevaluator = Evaluator.from_model(model, model_type=\"CLM\", device=\"cuda\")\nevaluator.evaluate_dataset(dataset, save_path=result_save_path, batch_size=32)\n</code></pre>"},{"location":"example/#inspect-bear-probe-results","title":"Inspect BEAR Probe results","text":"<p>We can load the BEAR probe results as follows:</p> <pre><code>from lm_pub_quiz import DatasetResults\nbear_results = DatasetResults.from_path(result_save_path)\n</code></pre>"},{"location":"example/#aggregate-results","title":"Aggregate Results","text":"<p>The DatasetResults object allows us to retrieve some aggregate results. Here we are loading the accuracy and the precision_at_k metrics: <pre><code>metrics = bear_results.get_metrics([\"accuracy\", \"num_instances\"])\n</code></pre> The method returns a pandas dataframe that holds the specified metrics for each relation (P6 to P7959) in the BEAR dataset (here showing the first five entries): <pre><code>     accuracy  num_instances relation_type\nP6   0.183333             60          None\nP19  0.206667            150          None\nP20  0.160000            150          None\nP26  0.050000             60          None\nP27  0.406667            150          None\n</code></pre></p> <p>To aggregate these accuracy scores over all relations we weigh them by the number of instances within each relation. Otherwise, greater accuracies more easily achieved on a small relation would inflate the overall accuracy.</p> <p><pre><code>import numpy as np\nweighted_accuracy = np.average(metrics.accuracy, weights=metrics.num_instances)\n</code></pre> For the gpt2 model we thus get a <code>weighted_accuracy</code> of <code>0.1495</code>. Note that this overall accuracy score is based only on the first template of each relation, which is the template considered by default by the <code>Evaluator</code>.</p>"},{"location":"example/#individual-results","title":"Individual Results","text":"<p>The DatasetResults object holds <code>RelationResult</code> objects for each relation in the probe that can be accessed using the relation codes in a key-like manner. If we want to take a more detailed look at the results for individual relations we may look at the instance tables these RelationResults hold:</p> <pre><code>relation_instance_table = bear_results[\"P36\"].instance_table\nprint(relation_instance_table.head())\n</code></pre> <p><pre><code>      sub_id                  sub_label  answer_idx                                         pll_scores  obj_id      obj_label\n3      Q1356                West Bengal           0  [-28.071779251, -35.064821243299996, -32.31778...   Q1348        Kolkata\n11     Q1028                    Morocco           1  [-33.614648819, -26.9230899811, -32.1363086701...   Q3551          Rabat\n15  Q3177715         Pagaruyung Kingdom           2  [-65.55403518690001, -67.46153640760001, -66.3...   Q3492        Sumatra\n18   Q483599  Southern Federal District           3  [-46.7988452912, -49.6077213287, -49.030160904...    Q908  Rostov-on-Don\n20    Q43684                      Henan           4  [-36.29014015210001, -37.7681064606, -41.59478...  Q30340      Zhengzhou\n</code></pre> Here we see the instance table for the relation <code>P36</code>. Each row of this instance table holds the results for a specific instance of this relation, i.e. the log-likelihood scores of instantiations of a template of the relation with the subject of this row and the objects in the relation answer space. The columns can be interpreted as follows:</p> <ul> <li><code>sub_id</code>: wikidata code of the subject instance of this row</li> <li><code>sub_label</code>: label of that subject instance</li> <li><code>sub_aliases</code>: alternative labels for that subject instance</li> <li><code>answer_idx</code>: id in the pll_scores list for the score of the true answer for this instance</li> <li><code>pll_scores</code>: (pseudo) log-likelihood scores for all objects in the answer space.</li> <li><code>obj_id</code>: wikidata code for the true object in the answer space</li> <li><code>obj_label</code>: label for the true object in the answer space</li> </ul> <p>Note that the <code>pll_scores</code> are ordered corresponding to the orders of the objects in this relations answer space (<code>bear_results[\"P36\"].answer_space</code>).</p> <p>We will lastly be looking at two examples of what we can do with this data: (1) Collect the specific instances the model got right for each relation. (2) Estimate the prior for each object in the answer space for each relation.</p>"},{"location":"example/#correct-instances","title":"Correct Instances","text":"<p>To gain more insight into the individual strengths and weaknesses of the model under investigation, we may want to inspect which specific instances of a relation the model got right and where it was wrong. Given the instance table this information is easy to retrieve. We only need to compare the index of the greatest pll_score to the <code>answer_idx</code> to determine whether for a given subject the correct object was scored as most likely:</p> <pre><code>relation_instance_table[\"correctly_predicted\"] = relation_instance_table.apply(lambda row: row.answer_idx == np.argmax(row.pll_scores), axis=1)\n</code></pre> <pre><code>      sub_id                  sub_label  answer_idx                                         pll_scores  obj_id      obj_label  correctly_predicted\n3      Q1356                West Bengal           0  [-28.071779251, -35.064821243299996, -32.31778...   Q1348        Kolkata                False\n11     Q1028                    Morocco           1  [-33.614648819, -26.9230899811, -32.1363086701...   Q3551          Rabat                 True\n15  Q3177715         Pagaruyung Kingdom           2  [-65.55403518690001, -67.46153640760001, -66.3...   Q3492        Sumatra                False\n18   Q483599  Southern Federal District           3  [-46.7988452912, -49.6077213287, -49.030160904...    Q908  Rostov-on-Don                False\n20    Q43684                      Henan           4  [-36.29014015210001, -37.7681064606, -41.59478...  Q30340      Zhengzhou                False\n</code></pre>"},{"location":"example/#answer-space-priors","title":"Answer Space Priors","text":"<p>Another question we may ask ourselves is to what extent a models log-likelihood scores for individual instantiations of a relation depend on what the model has learned about the connection between the specific subject and object or to what extent these scores are determined by a general bias the model possess towards certain objects in the answer space.</p> <p>To address this we may want to estimate the priors for all objects in the answer space.</p> <p>For a causal language model such as gpt2 the <code>pll_scores</code> are identical to the log-likelihood of the sentences derived by instantiating the template with the given subjects and objects. Taking the relation P30 as an example, with the template <code>[X] is located in [Y].</code>, the subject <code>Nile</code> and the object <code>Africa</code>, the <code>pll_score</code>for this pairing is the log of the probability assigned by the evaluated language model to the sentence <code>Nile is located in Africa</code>.</p> <p>Calculating the softmax over the <code>pll_scores</code> for a given subject of a relation gives us the conditional probabilities of the instantiated sentences of the relation conditioned on the fact that one of the instantiations is correct.</p> <p>Averaging these probability distributions over all subjects in the subject space of the relation estimates the priors for the objects in the answer space.</p> <pre><code>import torch\n\nrelation_code = \"P30\"\n\nsoftmax = torch.nn.Softmax(dim=0)\nrelation_instance_table = bear_results[relation_code].instance_table\nrelation_instance_table[\"pll_softmax\"] = relation_instance_table.pll_scores.apply(lambda x: softmax(torch.tensor(x)))\nrelation_priors = pd.Series(\n    torch.mean(torch.stack(list(relation_instance_table.pll_softmax)), dim=0),\n    index=bear_results[relation_code].answer_space.values\n)\n</code></pre> <p>For the relation P30 this results in the following priors.</p> <pre><code>Africa           0.162270\nAntarctica       0.111288\nAsia             0.120442\nEurope           0.220671\nNorth America    0.218366\nSouth America    0.166962\ndtype: float64\n</code></pre> <p>We can see that gpt2 is biased towards answering \"North America\" and \"Europa\" when assessing the entities in the subject space of relation P30 with the template <code>[X] is located in [Y].</code> (though the objects are balanced).</p>"},{"location":"intergration/","title":"Integration with Huggingface's Trainer","text":"<p>The lm_pub_quiz package allows for easy integration of the BEAR probe with the Huggingface Trainer. This way models can be continuously evaluated throughout training.</p>"},{"location":"intergration/#lm-pub-quiz-callback","title":"LM Pub Quiz Callback","text":"<p>Here is an example of how to set up the Trainer Callback needed for integration.</p> <p><pre><code># import the PubQuiz Trainer Callback class\nfrom lm_pub_quiz.integrations.transformers_trainer import PubQuizCallback\n\n# set up the trainer as you usually would\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\n# load the BEAR dataset\nbear_dataset = Dataset.from_path(\"../BEAR\")\n\n# create evaluator and Trainner callback\nevaluator = Evaluator.from_model(\n    model=model,\n    tokenizer=tokenizer,\n)\npub_quiz_callback = PubQuizCallback(\n    trainer=trainer,\n    evaluator=evaluator,\n    dataset=bear_dataset,\n    save_path=\"./bear_results\",\n)\n\n# add the PubQuiz callback to the Trainer\ntrainer.add_callback(pub_quiz_callback)\n\n# run training\ntrainer.train()\n</code></pre> Setting up the trainer prior to the callback is needed for the callback to have access to the logging functionality of the trainer. This way BEAR evaluation results are automatically logged by the trainer. And thus for example also included in a Tensorboard or Weights and Biases report.</p>"},{"location":"intergration/#optional-parameters","title":"Optional Parameters","text":"<p>The LM Pub Quiz Callback performs the BEAR probe automatically, whenever the evaluation strategy of the Trainer calls for it. It's behaviour can be further customized with a number of optional parameters:</p> Argument Description <code>save_path</code> If other than <code>None</code>, full BEAR evaluation results are saved to this directory. <code>accumulate</code> one of [None, \"domains\", \"cardinality\"] <code>batch_size</code> specifies the batch size for the Evaluator <code>template</code> either specify the template index a pass a list of template indices"},{"location":"intergration/#complete-example","title":"Complete Example","text":"<p>After the <code>training_run()</code> was completed you can view the reported metrics by calling <code>tensorboard --logdir logs/</code>. And inspect the full BEAR results saved at <code>&lt;PATH TO BEAR RESULTS SAVE DIR&gt;</code>. <pre><code>from datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForMaskedLM,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n)\nfrom lm_pub_quiz.integrations.transformers_trainer import PubQuizCallback\nfrom lm_pub_quiz.data import Dataset\nfrom lm_pub_quiz.evaluator import Evaluator\n\n\ndef training_run(\n        device=\"cpu\",\n        dataset_reduction_factor=0.002,\n):\n    # Load reduced dataset\n    dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-v1\")\n    select_train_indices = list(range(int(len(dataset['train']) * dataset_reduction_factor)))\n    select_validation_indices = list(range(int(len(dataset['validation']) * dataset_reduction_factor)))\n    dataset['train'] = dataset['train'].select(select_train_indices)\n    dataset['validation'] = dataset['validation'].select(select_validation_indices)\n\n    # load model and tokenizer\n    model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", truncation=True, max_length=512)\n\n    # do some minimal data preparation\n    def filter_text(example):\n        text = example['text']\n        if len(text) &lt; 100:\n            return False\n        if text[0] == '=' and text[-1] == '=':\n            return False\n        return True\n\n    dataset = dataset.filter(filter_text)\n\n    # tokenize data\n    def tokenize(example):\n        return tokenizer(example['text'], truncation=True, padding='max_length', max_length=512)\n\n    tokenized_train_ds = dataset['train'].map(tokenize, batched=True, remove_columns=['text'])\n    tokenized_eval_ds = dataset['validation'].map(tokenize, batched=True, remove_columns=['text'])\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)\n\n    # set up trainer with reporting to tensorboard\n    model_save_dir = './models/run_01'\n    training_args = TrainingArguments(\n        output_dir=model_save_dir,\n        run_name=\"run_01\",\n        num_train_epochs=1,\n        per_device_train_batch_size=2,\n        learning_rate=5e-01,\n        lr_scheduler_type='constant',\n        eval_steps=5,\n        eval_strategy=\"steps\",\n        logging_steps=5,\n        logging_dir='logs',\n        report_to='tensorboard',\n        push_to_hub=False,\n        save_strategy=\"no\",\n        include_num_input_tokens_seen=True,\n    )\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train_ds,\n        eval_dataset=tokenized_eval_ds,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n\n    # set up BEAR integration callback\n    dataset_path = \"&lt;PATH TO BEAR DATASET&gt;\"\n    bear_dataset = Dataset.from_path(dataset_path, relation_info=\"/home/seb/test_lm_pub_quiz/relation_info.json\")\n    bear_dataset = bear_dataset.filter_subset({\"P6\": list(range(5)), \"P30\": list(range(10)), \"P103\": list(range(5)), \"P175\": list(range(10))})\n    evaluator = Evaluator.from_model(\n        model=model,\n        tokenizer=tokenizer,\n        model_type=\"MLM\",\n        device=\"cpu\",\n    )\n    pub_quiz_callback = PubQuizCallback(\n        trainer=trainer,\n        evaluator=evaluator,\n        dataset=bear_dataset,\n        save_path=\"/home/seb/test_lm_pub_quiz/results\",\n        metrics=\"domains\",\n    )\n    trainer.add_callback(pub_quiz_callback)\n\n    # run training\n    trainer.train()\n    trainer.save_model()\n\ntraining_run()\n</code></pre></p>"}]}